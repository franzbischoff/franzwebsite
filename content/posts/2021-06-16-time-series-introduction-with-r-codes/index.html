---
title: Time Series Introduction with R codes
author: Francisco Bischoff
date: '2021-06-16'
categories:
  - R
  - r-bloggers
  - timeseries
tags:
  - R
slug: time-series-introduction-with-r-codes
bibliography: references.yaml
csl: springer-lncs.csl
mathjax: yes
editor_options:
  markdown:
    mode: markdown
output:
  blogdown::html_page:
    number_sections: yes
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="what-is-a-time-series" class="section level1" number="1">
<h1><span class="header-section-number">1</span> What is a Time Series</h1>
<ul>
<li><p>A set of observed values ordered in time, or we can say, repeated measurement of something usually with the same fixed interval of time (hourly, weekly, monthly).</p></li>
<li><p>A collection of observations made sequentially in time<span class="citation">[<a href="#ref-chatfield1996" role="doc-biblioref">1</a>]</span>.</p></li>
<li><p>If the variable we are measuring is a count variable, we may have a <a href="https://rpubs.com/franzbischoff/poisson_regression" target="_blank">Poisson Time Series</a> (that is for later).</p></li>
<li><p>A time series <span class="math inline">\(T \in \mathbb{R}^n\)</span> is a sequence of real-valued numbers <span class="math inline">\(t_i \in \mathbb{R} : T=[t_1,t_2,\dots,t_n]\)</span> where <span class="math inline">\(n\)</span> is the length of <span class="math inline">\(T\)</span>.</p></li>
</ul>
<p>Most of the classic statistical theory is based on the assumption of sample randomness and independent observations.
On the other hand, time series is just the opposite.
Observations are usually dependent on previous values, and their analysis must take into account their temporal order.</p>
<p>For example, a prospective cohort study comparing “injury rate before and after” an implemented program, analyses of time trends, such as Poisson regression and time series analysis, considers the variability that occurs over the study period apart from the change associated with the intervention.
They also avoid the loss of information about variability in incidence over time when rates are aggregated into one before and one after rate.
The population is its own control.</p>
<p>If previous observations can predict future observations exactly, we have a deterministic process.
However, the exact prediction is usually impossible since past values determine only part of the future value.
In this case, we say the process is stochastic, and the future values must be seen as a probability conditioned on the past values.</p>
<p>There are many models available to describe the behavior of a particular series.
The choice of such a model depends on factors such as the behavior of the phenomenon or the prior knowledge of its nature and the purpose of the analysis.</p>
<div id="types-of-time-series" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Types of Time Series</h2>
<p>The R base installation already gives us lots of datasets to work on time-series.
For this article I’ll first load the <code>MASS</code> package that contains some of the dataset we will use.
We can list all datasets available with the function <code>data()</code> from package <code>utils</code>.</p>
<pre class="r"><code>library(MASS)

data() # the output is not shown, but you can check in your RStudio</code></pre>
<ol style="list-style-type: decimal">
<li><p>Measured at regular time intervals (discrete), examples:</p>
<ul>
<li>Economic: stock market;</li>
</ul>
<pre class="r"><code>plot(EuStockMarkets[, 1], main = &quot;Daily closing prices of DAX&quot;, ylab = &quot;Price&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/type_economic-1.png" width="700" style="display: block; margin: auto;" /></p>
<ul>
<li>Physical/Biological: Pluviometry, DNA;</li>
</ul>
<pre class="r"><code>plot(((nhtemp) - 32) * 5 / 9, main = &quot;*Average* Yearly Temperatures in New Haven&quot;, ylab = &quot;Temperature in  ºC&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/type_physical-1.png" width="700" style="display: block; margin: auto;" /></p>
<ul>
<li>Marketing: sales per month;</li>
</ul>
<pre class="r"><code>plot(BJsales, main = &quot;Daily Sales Data (150 days)&quot;, ylab = &quot;Sales&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/type_marketing-1.png" width="700" style="display: block; margin: auto;" /></p>
<ul>
<li>Demographics: population per year, car accidents per day;</li>
</ul>
<pre class="r"><code>plot(austres, main = &quot;Quarterly Time Series of the Number of Australian Residents&quot;, ylab = &quot;Residents&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/type_demographics-1.png" width="700" style="display: block; margin: auto;" /></p>
<ul>
<li>Process control: factory measurements like final can weights, quality scores;</li>
</ul>
<pre class="r"><code>knitr::include_graphics(&quot;control_chart.png&quot;) # here I borrowed an image, sorry.</code></pre>
<p><img src="control_chart.png" style="display: block; margin: auto;" /></p>
<div style="font-size:70%;text-align:center">
<p>image from package <code>qicharts</code></p>
</div></li>
<li><p>Measured at irregular time intervals (events), examples:</p>
<ul>
<li>Point processes: earthquakes (events)</li>
</ul>
<pre class="r"><code>eq_data &lt;- readr::read_csv(&quot;https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_day.csv&quot;)
eq_data$yes &lt;- 0
plot(eq_data$time, eq_data$yes, main = &quot;USGS Magnitude 2.5+ Earthquakes, Past Day&quot;,
     ylab = &quot;Event&quot;, xlab = &quot;Time of the day&quot;, yaxt = &#39;n&#39;, sub = paste(&quot;Data from &quot;, Sys.Date()))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/type_earthquakes-1.png" width="700" style="display: block; margin: auto;" /></p>
<div style="font-size:70%;text-align:center">
<p>Data downloaded from <a href="https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_day.csv">usgs.gov</a></p>
</div></li>
<li><p>Measured continuously, examples:</p>
<ul>
<li>Binary processes: communication theory (turn-on/turn-off, zeros and ones);</li>
</ul>
<pre class="r"><code>set.seed(2114)
binary_data &lt;- unlist(replicate(10, rep(runif(1) &lt; 0.5, floor(runif(1, 10, 20)))))
binary_data &lt;- stepfun(seq_len(length(binary_data) - 1), binary_data)
plot(binary_data, main = &quot;Syntetic binary example&quot;, ylim = c(0,1),
     ylab = &quot;Value&quot;, xlab = &quot;Continuous time&quot;, do.points = FALSE, yaxp = c(0, 1, 1))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/type_binary-1.png" width="700" style="display: block; margin: auto;" /></p>
<ul>
<li>Analog signals: sound, temperature, humidity, ECG<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</li>
</ul>
<pre class="r"><code>ecg_example &lt;- readr::read_csv(&quot;ecg_example.csv&quot;) # this data you can find on physionet
plot.ts(ecg_example, main = &quot;ECG example&quot;, ylim = c(-4, 7), yaxp = c(-2, 4, 3),
     ylab = &quot;Value&quot;, xlab = &quot;Continuous time (ms)&quot;, xlim = c(0, 1000))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/type_ecg-1.png" width="700" style="display: block; margin: auto;" /></p>
<div style="font-size:70%;text-align:center">
<p>Data from <a href="https://www.physionet.org/content/challenge-2015/1.0.0/">Physionet</a>, record <code>a103l</code></p>
</div></li>
</ol>
</div>
</div>
<div id="the-goals-of-time-series-analysis" class="section level1" number="2">
<h1><span class="header-section-number">2</span> The goals of Time Series Analysis</h1>
<div id="description" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Description</h2>
<p>Like any kind of data analysis, the first step is to know the data.
It is imperative to plot a time series before trying to analyze it.
This simple step will show us any noticeable trend or seasonal variation and allow us to spot outliers<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> or some turning point in the data, which may require the use of more than one mode to fit each part of the data.</p>
</div>
<div id="explanation" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Explanation</h2>
<p>When we have multiple variables collected simultaneously, it may be possible to find some correlation between them.
The variation of one time series may explain the variation in another time series.
Multiple regression models may be helpful here.
We can also convert an input series into an output series by a linear operation and try to understand the relationship between both series.</p>
</div>
<div id="prediction" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Prediction</h2>
<p>Using the previously available data, we may want to predict the future values of that series.
Historically, the terms “prediction” and “forecasting” may be used interchangeably or not.
Thus it is essential to pay attention to how the literature is referring to both terms.
Sometimes “prediction” may refer to subjective methods or the procedure to achieve the “forecasting” (the objective method or the actual future values).
There is a close relationship between prediction and control problems where manufacturing processes that are going to move off-target can be proactively corrected.</p>
</div>
<div id="control" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Control</h2>
<p>When the time series is originated from measures of “quality” of a manufacturing process, the objective of the analysis is to <strong>control</strong> this process.
There are specific methods to do such control, and this topic is outside the scope of this article.
Further information is available on specific literature called Statistical Quality Control<span class="citation">[<a href="#ref-montgomery2012" role="doc-biblioref">3</a>]</span>.</p>
</div>
</div>
<div id="before-modeling-time-series" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Before Modeling Time Series</h1>
<div id="transformations" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Transformations</h2>
<ul>
<li>Stabilize variance: a logarithmic transformation may reduce the variance;</li>
</ul>
<pre class="r"><code># I&#39;ll explain this line once: this configures the output to hold two plots.
# And we store the old config. The last line is when we restore the config.
oldpar &lt;- par(mfrow = c(2, 1), mar = c(3.1, 4.1, 2.1, 1.1))

# Get only the Claims column and let&#39;s transform that in a time series, with anual frequency
claims &lt;- ts(Insurance$Claims, start = c(1973, 365.25*3/4), frequency = 365.25)
plot.ts(claims, main = &quot;Numbers of Car Insurance claims (1973, Q3)&quot;, ylab = &quot;Claims&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
plot(log(claims), ylab = &quot;log(Claims)&quot;) # here we log-transform the data

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/deal_variance-1.png" width="700" style="display: block; margin: auto;" /></p>
<ul>
<li>Make seasonality additive: a logarithmic transformation transforms a multiplicative seasonality into an additive one. However, this will only stabilize the variance if the error term is also multiplicative.</li>
</ul>
<pre class="r"><code>oldpar &lt;- par(mfrow = c(2, 1), mar = c(3.1, 4.1, 2.1, 1.1))

plot(AirPassengers, main = &quot;Monthly totals of international airline passengers&quot;, ylab = &quot;Passengers&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
plot(log(AirPassengers), ylab = &quot;log(Passengers)&quot;)

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/deal_additive-1.png" width="700" style="display: block; margin: auto;" /></p>
<ul>
<li>Normalize data distribution: data is usually assumed to be normal. Logarithmic and square-root are used; however, they are just special cases of the Box-Cox transformation. The parameters may be estimated by inference, and in general, the transformation cannot overcome all requirements at the same time.</li>
</ul>
<pre class="r"><code>layout(mat = matrix(c(1, 1, 2, 3), ncol = 2, byrow = TRUE)) # just plot format

plot.ts(airquality$Ozone, main = &quot;New York Air Quality Measurements, May to September 1973&quot;, ylab = &quot;Ozone&quot;, xlab = &quot;Time&quot;)
# Here we get the Ozone values, and remove the NA&#39;s so we can make &#39;statistics&#39;
ozone &lt;- ts(na.omit(airquality$Ozone))
hist(ozone, probability = 1); lines(density(ozone), col = &quot;red&quot;) # data distribution
hist(log(ozone), probability = 1); lines(density(log(ozone)), col = &quot;red&quot;) # fairly normalized</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/deal_normality-1.png" width="700" style="display: block; margin: auto;" /></p>
<p>It is interesting to note that Nelson and Granger, in 1979, found little benefit in applying a general Box-Cox transformation in several datasets.
Usually, it is advised to apply the least possible transformations, except when the variable has a direct physical interpretation.</p>
</div>
<div id="dealing-with-trends" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Dealing with Trends</h2>
<p>Trends in time series are difficult to define and have more than one “formal” definition in literature.
Loosely we can say that trend is a “long-term change in the mean level.” The main problem is how to define “long-term” in every situation.</p>
<p>The practical importance of a time series with a trend depends on whether we want to <strong>measure the trend</strong> and/or <strong>remove the trend</strong> so we can analyze the higher frequency oscillations that remain.</p>
<p>It is important to remember that sometimes we may be more interested in the trend than what is left after removing it.</p>
<div id="curve-fitting" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Curve Fitting</h3>
<p>This technique is not more than removing the trend and analyze the “residuals.”
In general, particularly for yearly data, we can use a simple polynomial curve.</p>
<pre class="r"><code>oldpar &lt;- par(mfrow = c(2, 1), mar = c(3.1, 4.1, 2.1, 1.1))

# Our old friend, AirPassengers dataset. See how it increases both the mean and the variance.
plot(AirPassengers, main = &quot;Monthly totals of international airline passengers&quot;, ylab = &quot;Passengers&quot;)
# Let&#39;s just fit a line on the data
fit &lt;- lm((AirPassengers) ~ seq_along(AirPassengers))
pred &lt;- predict(fit, data.frame(seq_along(AirPassengers)))
pred &lt;- ts(pred, start = start(AirPassengers), frequency = frequency(AirPassengers))
# `pred` contains our line based on the simple linear model we did.
data &lt;- AirPassengers / pred # why divide and not subtract? because the variance also increases (we have a multiplicative seasonality)
lines(pred, col = &quot;red&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
plot(data, ylab = &quot;Passengers / fitted&quot;)

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/trends_curve_fitting-1.png" width="700" style="display: block; margin: auto;" /></p>
</div>
<div id="filtering" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Filtering</h3>
<p>Filtering is a little more complex operation than curve fitting.
We are not just trying to find a polynomial that best fits our data, but we are transforming our original TS into another TS using a formula (that here we call filter).
This filter can be one of several kinds of “Moving Averages,” locally weighted regressions (e.g., LOESS), or “Splines” (a piecewise polynomial).
One caveat of these smoothing techniques is the end-effect problem (since in one end of the time series, we do not have all the values to compute, for example, the moving average).</p>
<p>Simple moving average ex: <span class="math inline">\(Sm(x_t) = \frac{1}{2q+1} \sum^{+q}_{r = -q}x_{t+r}\)</span></p>
<p>Example with splines:</p>
<pre class="r"><code>oldpar &lt;- par(mfrow = c(3, 1), mar = c(3.1, 4.1, 2.1, 1.1))

plot(USAccDeaths, main = &quot;Accidental Deaths in the US 1973-1978&quot;, ylab = &quot;Accidental Deaths&quot;)

# here is another way to &quot;fit a line&quot;. The number of knots is your choice, but an old Professor once told that 4-5 per year is sufficient.
pred &lt;- smooth.spline(USAccDeaths, nknots = 24)$y
pred &lt;- ts(pred, start = start(USAccDeaths), frequency = frequency(USAccDeaths))
lines(pred, col = &quot;red&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
data &lt;- USAccDeaths - pred # see how subtraction and division only affects the mean on this case.
plot(data, ylab = &quot;Deaths - spline&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
data &lt;- USAccDeaths / pred
plot(data, ylab = &quot;Deaths / spline&quot;)

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/trends_curve_fitting2-1.png" width="700" style="display: block; margin: auto;" /></p>
</div>
<div id="differencing" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Differencing</h3>
<p>It is a special kind of filtering, where we compute the difference between the current value and the next.
It is helpful to remove trends, making a TS stationary.
We can use differencing multiple times (we call “orders”), but usually, one (“first-order”) iteration is sufficient.
The mathematical operator used to denote differencing is the “nabla” (<span class="math inline">\(\nabla\)</span>).
<span class="math inline">\(\nabla^2\)</span> means second-order differencing.</p>
<pre class="r"><code>oldpar &lt;- par(mfrow = c(2, 1), mar = c(3.1, 4.1, 2.1, 1.1))

plot(co2, main = &quot;Mauna Loa Atmospheric CO2 Concentration&quot;, ylab = &quot;CO2 concentration&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
plot(diff(co2), ylab = &quot;diff(co2)&quot;) # the first difference removes all the trend!

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/trends_diff-1.png" width="700" style="display: block; margin: auto;" /></p>
<p>See that the example above removed the trend, but kept the seasonality.</p>
<blockquote>
<p><strong>The Slutzky-Yule effect</strong><span class="citation">[<a href="#ref-loynes2005" role="doc-biblioref">2</a>]</span>: They showed that by using operations like differencing and moving average, one could <strong>induce</strong> sinusoidal variation in the data that, in fact, is not real information.</p>
</blockquote>
</div>
</div>
<div id="dealing-with-seasons" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Dealing with Seasons</h2>
<p>As for trends, the analysis of seasonal variation depends on whether we want to <strong>measure the seasonal effect</strong> and/or <strong>remove the seasonality</strong>.</p>
<p>For a time series with a slight trend, a straightforward estimate of the seasonal effect is to take the average of every January (for example) and subtract (in additive case) or divide by (in multiplicative case) the average of the year.</p>
<p>For a time series with a significant trend, a more robust approach may be taken.
For monthly data, we can use:</p>
<p><span class="math display">\[
Sm(x_t) = \frac{\frac{1}{2}x_{t-6} + x_{t-5} + x_{t-4} + \cdots + x_{t+5} + \frac{1}{2}x_{t+6}}{12}
\]</span></p>
<p>The seasonality can also be eliminated by differencing with lag.
For example, with monthly data, we can use the operator <span class="math inline">\(\nabla_{12}\)</span>:</p>
<pre class="r"><code>oldpar &lt;- par(mfrow = c(2, 1), mar = c(3.1, 4.1, 2.1, 1.1))

plot(USAccDeaths, main = &quot;Accidental Deaths in the US 1973-1978&quot;, ylab = &quot;Accidental Deaths&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
# here still a first degree differencing, but with a leap of 12 months.
# See how we remove the seasonality, but not the trend.
plot(diff(USAccDeaths, lag = 12), ylab = &quot;diff(lag = 12)&quot;)

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/seasons_diff-1.png" width="700" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[
\nabla_{12}x_t=x_t-x_{t-12}
\]</span></p>
<p>See that the example above removed the seasonality, but kept the trend.</p>
</div>
<div id="autocorrelation" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Autocorrelation</h2>
<p>It may be the fundamental property of a time series.
As pointed before in <a href="#what-is-a-time-series">section 1</a>, time series data has the assumption of non-independence, and autocorrelation is just how we turn this property into an objective and measurable value.
Autocorrelation measures the correlation between observations at different time lags.
As we may observe, at time zero, the coefficient is one because the observed value totally agrees with itself, and sometimes this coefficient is skipped in some plots.
The distribution of these coefficients provides us insight into the probability model that had generated this series.
Later in <a href="#stationary-processes">section 4.1</a>, we will write the definition of this property.</p>
<div id="the-correlogram" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> The correlogram</h3>
<p>The graphic representation of the autocorrelation coefficients is called a correlogram, in which the coefficient <span class="math inline">\(r\)</span> is plotted against the lag <span class="math inline">\(k\)</span>.</p>
<p>Interpretation.
Here we offer some general advice:</p>
<ul>
<li><p><strong>Random series:</strong> for a large <span class="math inline">\(N\)</span>, <span class="math inline">\(r_k \simeq 0\)</span> for all non-zero values of <span class="math inline">\(k\)</span>.
Usually, 19 out of 20 of the values of <span class="math inline">\(r_k\)</span> lie between <span class="math inline">\(\pm 2/\sqrt{N}\)</span>.</p></li>
<li><p><strong>Short-term correlation:</strong> Stationary series usually presents with a short-term correlation.
What we see is a large value for <span class="math inline">\(r_1\)</span>, followed by a geometric decay.</p></li>
<li><p><strong>Alternating series:</strong> If the data tends to alternate sequentially around the overall mean, the correlogram will also show this behavior: the value of <span class="math inline">\(r_1\)</span> will be negative, <span class="math inline">\(r_2\)</span> will be positive, and so on.</p></li>
<li><p><strong>Non-stationary series:</strong> If the data has a trend, the values of <span class="math inline">\(r_k\)</span> will not come to zero, only for large lag values.
This kind of correlogram has little to offer since the trend muffles any other features we may be interested in.
Here we can conclude that the correlogram is only helpful after removing any trend (in other words, turn the series stationary).</p></li>
<li><p><strong>Seasonal fluctuations:</strong> If the data contains seasonal fluctuations, the correlogram will display an oscillation of the same frequency.</p></li>
</ul>
</div>
<div id="testing-for-randomness" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Testing for randomness</h3>
<p>There are valuable tools to test if the data is random or not.
For the sake of simplicity, this subject will not be covered in this article.
However, testing residuals for randomness is a different problem<span class="citation">[<a href="#ref-chatfield1996" role="doc-biblioref">1</a>]</span> and will be discussed later.</p>
</div>
</div>
</div>
<div id="stochastic-processes" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Stochastic Processes</h1>
<p>In the real world, most processes have in their structure a random component.
The term “stochastic” is a Greek word that means “pertaining to chance.” A more formal definition of a stochastic process is: “a collection of random variables which are ordered in time and defined at a set of time points which may be continuous or discrete”<span class="citation">[<a href="#ref-chatfield1996" role="doc-biblioref">1</a>]</span>.</p>
<p>Most statistical problems are focused on estimating the properties of a population from a sample.
In time series, we need to realize that each data point is a “sample” of the “population” at that given time.
When we read a value from a sensor (for a plausible example, let us think of an arterial line that measures the blood pressure directly), we read a unique value at that time, not the distribution of values that could be possible read.
This infinite set of possible values that could compose our time series is called an <strong>ensemble.</strong> The actual time series we have is one of the possibly <strong>realizations</strong> of the stochastic process.</p>
<p>As with other mathematical functions, a simple way to describe the stochastic process (as probability function) is using its <a href="https://en.wikipedia.org/wiki/Moment_(mathematics)">moments</a>.
The first moment is the mean, and the second moment is the variance (and the autocovariance, for a sequence of random variables).</p>
<div id="stationary-processes" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Stationary Processes</h2>
<p>It is the process that is ready to model.
In other words, the previous steps before modeling a time series are to make it stationary.
Loosely speaking, a stationary process is a process that has a constant mean, variance, and no periodic variations.</p>
<p>Formally, a we can say a process is strictly stationary if the joint distribution of <span class="math inline">\(X(t_1), \dots, X(t_n)\)</span> is the same as the joint distribution of <span class="math inline">\(X(t_1 + \tau), \dots, X(t_n+ \tau) \;\; \text{for all} \;\; t_1,\dots,t_n,\tau\)</span>.
Strict stationarity implies that</p>
<p><span class="math display">\[
\mu(t)=\mu \\
\sigma^2(t) = \sigma^2
\]</span></p>
<p>are constants independently of the value of <span class="math inline">\(t\)</span>.
In addition, the joint distribution of <span class="math inline">\(X(t_1)\)</span> and <span class="math inline">\(X(t_2)\)</span> depends only on <span class="math inline">\((t_2-t_1)\)</span>, which is called the <strong>lag</strong>.
Thus the autocovariance function (ACVF) <span class="math inline">\(\gamma(t_1, t_2)\)</span> also depends only on <span class="math inline">\((t_2-t_1)\)</span> and may be written as <span class="math inline">\(\gamma(\tau)\)</span> (the autocovariance coefficient at lag <span class="math inline">\(\tau\)</span>).</p>
<p>As the autocovariance coefficient depends on the units in which <span class="math inline">\(X(t)\)</span> is measured, the <strong>ACVF</strong> is standardized to what is called the <strong>autocorrelation</strong> function (<strong>ACF</strong>), which is given by</p>
<p><span class="math display">\[
\rho(\tau)=\gamma(\tau)/\gamma(0)
\]</span></p>
<p>which measures the correlation between <span class="math inline">\(X(t)\)</span> and <span class="math inline">\(X(t+\tau)\)</span>.</p>
<p>The reasoning behind the suggestion that the distribution of <span class="math inline">\(X(t)\)</span> should be the same for all <span class="math inline">\(t\)</span> resides in the fact that many processes that converge to an <strong>equilibrium</strong> as <span class="math inline">\(t \rightarrow \infty\)</span>, which the probability distribution of <span class="math inline">\(X(t)\)</span> does <strong>not</strong> depend on the initial conditions.
With this assumption, after the process has been running for some time, the probability distribution of <span class="math inline">\(X(t)\)</span> will change very little.</p>
<p>Strict stationarity is very restrictive and few processes achieve it.</p>
<div id="second-order-stationarity" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Second-order Stationarity</h3>
<p>A more helpful definition, for practical reasons, is a less restricted definition of stationarity where the mean is constant, and <strong>ACVF</strong> only depends on the lag.
This process is called second-order stationary.</p>
<p>This simplified definition of stationarity will be generally as long as the properties of the processes depend only on its structure as specified by its <a href="#stochastic-processes">first and second moments</a>.</p>
</div>
</div>
<div id="the-autocorrelation-function" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> The autocorrelation function</h2>
<p>As shown in <a href="#the-correlogram">section 3.4.1</a>, the autocorrelation coefficients are helpful in describing a time series.
The autocorrelation function (<strong>ACF</strong>) is an essential tool for assessing its properties.</p>
<p>Here we will describe the properties of the <strong>ACF</strong>.</p>
<p>Suppose a stationary stochastic process <span class="math inline">\(X(t)\)</span> has mean <span class="math inline">\(\mu\)</span>, variance <span class="math inline">\(\sigma^2\)</span>, <strong>ACVF</strong> <span class="math inline">\(\gamma(\tau)\)</span>, and <strong>ACF</strong> <span class="math inline">\(\rho(\tau)\)</span>.
Then</p>
<p><span class="math display">\[
\rho(\tau)=\gamma(\tau)/\gamma(0) = \gamma(\tau)/\sigma^2 \qquad \text{for } \rho(0)=1
\]</span></p>
<div id="property-1" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Property 1</h3>
<p>The <strong>ACF</strong> is an <strong>even</strong> function of the lag in that</p>
<p><span class="math display">\[
\rho(\tau)=\rho(-\tau)
\]</span></p>
<p>This property just states that the correlation between <span class="math inline">\(X(t)\)</span> and <span class="math inline">\(X(t + \tau)\)</span> is the same as that between <span class="math inline">\(X(t)\)</span> and <span class="math inline">\(X(t-\tau)\)</span>.
The result is easily proved using <span class="math inline">\(\gamma(\tau)=\rho(\tau)\sigma^2\)</span> by</p>
<p><span class="math display">\[
\begin{aligned}
\gamma(\tau)&amp;=Cov[X(t), X(t+\tau)] \\
            &amp;=Cov[X(t-\tau), X(t)] \qquad \text{since} \; X(t) \; \text{stationary} \\
            &amp;=\gamma(-\tau)
\end{aligned}
\]</span></p>
</div>
<div id="property-2" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Property 2</h3>
<p><span class="math inline">\(|\rho(\tau)| \leq 1\)</span>.
This is the “usual” property of a correlation.
It is proved by noting that</p>
<p><span class="math display">\[
Var[\lambda_1 X(t) + \lambda_2 X(t+\tau)] \geq 0
\]</span></p>
<p>for any constants <span class="math inline">\(\lambda_1, \lambda_2\)</span> since variance is always non-negative.
This variance is equal to</p>
<p><span class="math display">\[
\lambda_1^2 Var[X(t)] + \lambda_2^2 Var[X(t + \tau)] + 2 \lambda_1 \lambda_2 Cov[X(t), X(t + \tau)] = (\lambda_1^2 + \lambda_2^2) \sigma^2 + 2 \lambda_1 \lambda_2 \gamma(\tau)
\]</span></p>
<p>When <span class="math inline">\(\lambda_1 = \lambda_2 = 1\)</span>, we find</p>
<p><span class="math display">\[
\gamma(\tau) \ge -\sigma^2
\]</span></p>
<p>so that <span class="math inline">\(\rho(\tau) \ge -1\)</span>.
When <span class="math inline">\(\lambda_1 = 1, \lambda_2 = -1\)</span>, we find</p>
<p><span class="math display">\[
\sigma^2 \ge \gamma(\tau)
\]</span></p>
<p>so that <span class="math inline">\(\rho(\tau) \le +1\)</span></p>
</div>
<div id="property-3" class="section level3" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Property 3</h3>
<p>Lack of uniqueness.
A stochastic process has a unique covariance structure.
However, the opposite is not valid.
We can find other processes that produce the same <strong>ACF</strong>, adding another level of difficulty to the sample <strong>ACF</strong> interpretation.
To overcome this problem, we have the invertibility condition that will be described later on <a href="#moving-average-processes">Moving average processes</a>.</p>
</div>
</div>
<div id="some-useful-stochastic-processes" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Some useful stochastic processes</h2>
<div id="purely-random-process" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Purely random process</h3>
<p>A process can be called purely random if it is composed of a sequence of random variables <span class="math inline">\(\{Z_t\}\)</span> that are independent and identically distributed.
By definition, it has constant mean and variance, given that</p>
<p><span class="math display">\[
\gamma(k)=Cov(Z_t, Z_{t+k}) = 0 \qquad \text{for}\;k=\pm 1, 2, \dots
\]</span></p>
<p>Since the mean and <strong>ACVF</strong> do not depend on time, the process is second-order stationary.
In fact, it also satisfies the condition for a strictly stationary process.
The <strong>ACF</strong> is given by</p>
<p><span class="math display">\[
\rho(k)=\left\{
\begin{array}{ll}
1 &amp; k=0 \\
0 &amp; k=\pm 1, \pm 2, \ldots
\end{array}\right.
\]</span></p>
<pre class="r"><code>set.seed(2021)
oldpar &lt;- par(mfrow = c(2, 1), mar = c(3.1, 4.1, 2.1, 1.1))

normal_random &lt;- rnorm(500)
plot.ts(normal_random, main = &quot;Purely random series - N(0, 1)&quot;, ylab = &quot;Value&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
acf(normal_random, lag.max = 1000)

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/acf_random-1.png" width="700" style="display: block; margin: auto;" /></p>
<div style="font-size:70%;text-align:center">
<p>In the example above, the <strong>ACF</strong> function was kept along the entire dataset for academic purposes.
Normally it is shown only the <span class="math inline">\(10*log10(N/m)\)</span> lags, where <span class="math inline">\(N\)</span> is the number of observations and <span class="math inline">\(m\)</span> the number of series (from <code>acf {stats}</code> manual).</p>
</div>
<p>This type of process is of particular importance as building blocks of more complicated processes such as moving average processes.</p>
</div>
<div id="random-walk" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Random walk</h3>
<p>The Random Walk is a process very similar to the previous process.
The difference lies in that the current observation sums the current random variable to the previous observation instead of being independent.
The definition is given by</p>
<p><span class="math display">\[
X_t=X_{t-1}+Z_t
\]</span></p>
<p>Usually, the process starts at zero for <span class="math inline">\(t=0\)</span>, so that</p>
<p><span class="math display">\[
X_1=Z_1
\]</span></p>
<p>and</p>
<p><span class="math display">\[
X_t= \sum_{i=1}^t Z_i
\]</span></p>
<p>Then we find that <span class="math inline">\(E(X_t)=t\mu\)</span> and that <span class="math inline">\(Var(X_t)=t\sigma^2_Z\)</span>.
As the mean and variance change with <span class="math inline">\(t\)</span>, the process is non-stationary.</p>
<pre class="r"><code>set.seed(2021)

random_walk &lt;- cumsum(rnorm(500))
plot.ts(random_walk, main = &quot;Random Walk - N(0, 1)&quot;, ylab = &quot;Value&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/stoch_rwalk-1.png" width="700" style="display: block; margin: auto;" /></p>
<p>Meanwhile, the interesting feature is that the first difference of a random walk forms a purely random process, which is therefore stationary.</p>
<pre class="r"><code>set.seed(2021)

random_norm &lt;- rnorm(500)
random_walk &lt;- cumsum(random_norm) # same from the last plot
diff_walk &lt;- diff(random_walk)
# below we use 2:500 because with diff, we lose the first observation
all.equal(diff_walk, random_norm[2:500])
## [1] TRUE</code></pre>
</div>
<div id="moving-average-processes" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Moving average processes</h3>
<p>First, not to be confused with the <strong>moving average</strong> algorithm.
The moving average process is a common approach to model a univariate time series.
The concept is that the current value <span class="math inline">\(X_t\)</span> depends <strong>linearly</strong> on <span class="math inline">\(q\)</span> past values of a stochastic process.
Another practical way to see this process is imagining the process as a finite impulse applied to a white noise.
This impulse “has” affected the <span class="math inline">\(q\)</span> previous values and the current.</p>
<p>The moving average process only remembers the <span class="math inline">\(q\)</span> previous components of the random process<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>, so it is also limited to <span class="math inline">\(q\)</span> steps in the future.
After that, one cannot predict any value without new random values being generated<span class="citation">[<a href="#ref-siegel2016" role="doc-biblioref">4</a>]</span>.</p>
<p>Here we will say that <span class="math inline">\(\{Z_t\}\)</span> is a process that only generates purely random values with mean zero and variance <span class="math inline">\(\sigma^2_Z\)</span>.
Then the process <span class="math inline">\(\{X_t\}\)</span> can be said to be a moving average process of order <span class="math inline">\(q\)</span> (abbreviated to a <span class="math inline">\(\text{MA}(q)\)</span> process) if</p>
<p><span class="math display">\[
X_t=\beta_0Z_t+\beta_1Z_{t-1}+\cdots+\beta_qZ_{t-q} \tag{4.4.3.1}
\]</span></p>
<p>where <span class="math inline">\(\{\beta_i\}\)</span> are constants.</p>
<blockquote>
<p>This equation is very similar to a linear regression <span class="math inline">\(y = a + bx\)</span> where the dependent process <span class="math inline">\(\{X_t\}\)</span> is modeled by an independent process <span class="math inline">\(\{Z_t\}\)</span> (a purely random process).
Here we omit, for simplicity, the “intercept,” that may be a constant <span class="math inline">\(\mu\)</span> added to the end of the right side of the equation.
The “intercept” is the overall mean of this process.</p>
</blockquote>
<p>Usually, the random process is scaled so that <span class="math inline">\(\beta_0=1\)</span>.
Then we find that</p>
<p><span class="math display">\[
E(X_t)=0 \\
Var(X_t)=\sigma^2_Z \sum^q_{i=0} \beta_i^2
\]</span></p>
<p>since the <span class="math inline">\(Z\)</span>s are independent.
We also have</p>
<p><span class="math display">\[
\begin{aligned}
\gamma(k) &amp;= Cov(X_t, X_{t+k}) \\
    &amp;= Cov(\beta_0Z_t+\cdots+\beta_qZ_{t-q}, \beta_0Z_{t-k}+\cdots+\beta_qZ_{t+k-q}) \\
    &amp;= \left\{\begin{array}{cc}
        0 &amp; k&gt;q \\
        \sigma_{Z}^{2} \sum_{i=0}^{q-k} \beta_{i} \beta_{i+k} &amp; k=0,1, \ldots, q \\
        \gamma(-k) &amp; k&lt;0
        \end{array}\right.
\end{aligned}
\]</span></p>
<p>since</p>
<p><span class="math display">\[
Cov(Z_{s}, Z_{t})=\left\{\begin{array}{ll}
    \sigma_{Z}^{2} &amp; s=t \\
    0 &amp; s \neq t
    \end{array}\right.
\]</span></p>
<p>As <span class="math inline">\(\gamma(k)\)</span> does not depend on <span class="math inline">\(t\)</span>, and the mean is constant, the process is second-order stationary for all values of the <span class="math inline">\(\{\beta_i\}\)</span>.
Furthermore, if the <span class="math inline">\(Z\)</span>s are normally distributed, so are the <span class="math inline">\(X\)</span>s, and we have a strictly stationary normal process.</p>
<p>The <strong>ACF</strong> of the <span class="math inline">\(\text{MA}(q)\)</span> process is given by</p>
<p><span class="math display">\[
\rho(k)=\left\{\begin{array}{cl}
1 &amp; k=0 \\
\sum_{i=0}^{q-k} \beta_{i} \beta_{i+k} / \sum_{i=0}^{q} \beta_{i}^{2} &amp; k=1, \ldots, q \\
0 &amp; k&gt;q \\
\rho(-k) &amp; k&lt;0
\end{array}\right.
\]</span></p>
<p>Note that the <strong>ACF</strong> is “clipped” to zero at lag <span class="math inline">\(q\)</span>.
This is a feature of <span class="math inline">\(\text{MA}\)</span> processes that we can spot on <strong>ACF</strong> plots.
In practice, the “zero” will be a value below the significance line.</p>
<pre class="r"><code>oldpar &lt;- par(mfrow = c(3, 1), mar = c(3.1, 4.1, 2.1, 1.1))
set.seed(2021)

mov_avg &lt;- arima.sim(list(order = c(0,0,1), ma = 0.8), n = 200)
plot.ts(mov_avg, main = &quot;Moving Average MA(1)&quot;, ylab = &quot;Value&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
acf(mov_avg)
par(mar = c(5.1, 4.1, 0.1, 1.1))
pacf(mov_avg)

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/stoch_movavg-1.png" width="700" style="display: block; margin: auto;" /></p>
<p>There is no restriction on <span class="math inline">\(\{\beta_i\}\)</span> values to produce a stationary <span class="math inline">\(\text{MA}\)</span> process.
However, as we briefly mentioned at the <strong>ACF</strong> <a href="#property-3">Property 3</a>, it is desirable that the process is <strong>invertible</strong> (e.g., Box and Jenkins, 1970, p. 50).</p>
<div id="first-order-process" class="section level4" number="4.3.3.1">
<h4><span class="header-section-number">4.3.3.1</span> First-order process</h4>
<p>The invertibility issue is shown below, where two different <span class="math inline">\(\text{MA}(1)\)</span> processes results in the same <strong>ACF</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\text { A } \quad X_{t}=Z_{t}+\theta Z_{t-1}\\
&amp;\text { B } \quad X_{t}=Z_{t}+\frac{1}{\theta} Z_{t-1}
\end{aligned}
\]</span></p>
<p>We can see the problem better if we express those processes putting <span class="math inline">\(Z_t\)</span>, in terms of <span class="math inline">\(X_t,X_{t-1},\dots\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\text { A } \quad Z_{t}=X_{t}-\theta X_{t-1}+\theta^{2} X_{t-2}-\cdots\\
&amp;\text { B } \quad Z_{t}=X_{t}-\frac{1}{\theta} X_{t-1}+\frac{1}{\theta^{2}} X_{t-2}-\cdots
\end{aligned}
\]</span></p>
<p>In this form, if <span class="math inline">\(|\theta| &lt; 1\)</span>, the process <strong>A</strong> converges whereas the process <strong>B</strong> does not.
Thus if <span class="math inline">\(|\theta| &lt; 1\)</span>, the process <strong>A</strong> is said to be invertible, whereas the process <strong>B</strong> is not.
This assures that there will be only one <span class="math inline">\(\text{MA}\)</span> process for each <strong>ACF</strong> (uniqueness)</p>
<p>To simplify the expression satisfying the invertibility condition, we can use the backward shift operator <span class="math inline">\(B\)</span>, which is defined by</p>
<p><span class="math display">\[
B^jX_t=X_{t-j} \qquad \text{for all } j
\]</span></p>
<p>Then equation (4.4.3.1) may be written as</p>
<p><span class="math display">\[
\begin{aligned}
X_t &amp;= (\beta_0+\beta_1B+\cdots+\beta_qB^q)Z_t \\
    &amp;= \theta(B)Z_t
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\theta(B)\)</span> the polynomial representation of the power series of order <span class="math inline">\(q\)</span> in <span class="math inline">\(B\)</span>.
A <span class="math inline">\(\text{MA}\)</span> process of order <span class="math inline">\(q\)</span> <strong>is invertible if the roots of the equation</strong> (regarding <span class="math inline">\(B\)</span> as a complex variable and not an operator) <strong>all lie outside the unit circle</strong> (Box and Jenkins, 1970, p. 50)</p>
<p><span class="math display">\[
\theta(B)=\beta_0+\beta_1B+\cdots+\beta_qB^q=0
\]</span></p>
<p>For example, in the first-order case, <span class="math inline">\(\text{MA}(1)\)</span>, we have <span class="math inline">\(\theta(B)=1+\theta B\)</span>, which has root <span class="math inline">\(B=-1/\theta\)</span>.
Thus the root is outside the unit circle provided that <span class="math inline">\(|\theta| &lt; 1\)</span><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
</div>
<div id="general-order-case" class="section level4" number="4.3.3.2">
<h4><span class="header-section-number">4.3.3.2</span> General-order case</h4>
<p>We can also extend the concept to the general case of <span class="math inline">\(\text{MA}(q)\)</span>, where we can decompose de polynomial <span class="math inline">\(\theta(B)\)</span> as <span class="math inline">\(\theta(B) = (1 +\theta_1B)\cdots(1 +\theta_qB)\)</span>.
In this case, if all the roots <span class="math inline">\(-1/\theta_1, \dots, -1/\theta_q\)</span> shall lie outside the unit circle, so the process is invertible.</p>
</div>
</div>
<div id="autoregressive-processes" class="section level3" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Autoregressive processes</h3>
<p>In the previous section about the moving average process, we imagined the process as an experiment where you had an impulse applied to a random process with a finite time span influence.
Here we can think of an experiment where this impulse persists in time, but its influence is not visible immediately, but we see it as a repeatable pattern over and over.
This also implies that the <span class="math inline">\(\text{AR}\)</span> process may not be stationary, in contrast with <span class="math inline">\(\text{MA}\)</span> process.
Moreover, <span class="math inline">\(\text{MA}\)</span> process only “remembers” the previous components of the underlying random process, where the <span class="math inline">\(\text{AR}\)</span> process depends directly on the previous observations, hence the prefix “auto” regressive.</p>
<p>Again, we will say that <span class="math inline">\(\{Z_t\}\)</span> is a process that only generates purely random values with mean zero and variance <span class="math inline">\(\sigma^2_Z\)</span>.
Then the process <span class="math inline">\(\{X_t\}\)</span> can be said to be an autoregressive process of order <span class="math inline">\(p\)</span> if</p>
<p><span class="math display">\[
X_t=\alpha_1 X_{t-1} + \cdots + \alpha_p X_{t-p} + Z_t \tag{4.3.4.1}
\]</span></p>
<p>In contrast with the <span class="math inline">\(\text{MA}\)</span> process, the <span class="math inline">\(\text{AR}\)</span> process looks like a multiple regression model since <span class="math inline">\(X_t\)</span> is regressed not on independent variables but past values of <span class="math inline">\(X_t\)</span>.
An autoregressive process of order <span class="math inline">\(p\)</span> will be abbreviated to an <span class="math inline">\(\text{AR}(p)\)</span> process.</p>
<div id="first-order-process-1" class="section level4" number="4.3.4.1">
<h4><span class="header-section-number">4.3.4.1</span> First-order process</h4>
<p>For a better understanding, we will analyze the first-order case, for <span class="math inline">\(p=1\)</span>.
Then</p>
<p><span class="math display">\[
X_t=\alpha X_{t-1} + Z_t \tag{4.3.4.1.1}
\]</span></p>
<p>The <span class="math inline">\(\text{AR}(1)\)</span> is a special case of the <a href="https://setosa.io/blog/2014/07/26/markov-chains/index.html">Markov process</a>, named after the Russian Andrey Markov.
By successive substitution in equation (4.4.4.1.1) we may write</p>
<p><span class="math display">\[
\begin{aligned}
X_t &amp;= \alpha(\alpha X_{t-2} + Z_{t-1}) + Z_t \\
    &amp;= \alpha^2(\alpha X_{t-3} + Z_{t-2}) + \alpha Z_{t-1} + Z_t
\end{aligned}
\]</span></p>
<p>and eventually, we find that <span class="math inline">\(\{X_t\}\)</span>, can be represented as an <strong>infinite-order</strong> <span class="math inline">\(\text{MA}\)</span> process as</p>
<p><span class="math display">\[
X_t= Z_t + \alpha Z_{t-1} + \alpha^2 Z_{t-2} + \cdots \qquad \text{provided} \; -1 &lt; \alpha &lt; +1 \tag{4.3.4.1.2}
\]</span></p>
<p>This duality between <span class="math inline">\(\text{AR}\)</span> and <span class="math inline">\(\text{MA}\)</span> processes is useful for a variety of purposes.
The same transformation can be accomplished using the backward shift operator <span class="math inline">\(B\)</span>.
Then equation (4.3.4.1.1) may be written</p>
<p><span class="math display">\[
(1 - \alpha B) X_t = Z_t
\]</span></p>
<p>so that<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a><span class="math display">\[
\begin{aligned}
X_t &amp;= Z_t/(1-\alpha B) \\
    &amp;= (1 + \alpha B + \alpha^2 B^2 + \cdots) Z_t \\
    &amp;= Z_t + \alpha Z_{t-1} + \alpha^2 Z_{t-2} + \cdots \qquad \text{same as eq. (4.3.4.1.2)}
\end{aligned}
\]</span></p>
<p>Comparing with the previous solution for moving average process, we see that<span class="math display">\[
\begin{aligned}
E(X_t) &amp;= 0 \\
Var(X_t) &amp;= \sigma^2_Z(1+ \alpha^2 + \alpha^4 + \cdots)
\end{aligned}
\]</span></p>
<p>The variance is finite if we assume that <span class="math inline">\(|\alpha| &lt; 1\)</span>, in which case</p>
<p><span class="math display">\[
Var(X_t) = \sigma^2_X = \sigma^2_Z/(1- \alpha^2)
\]</span></p>
<p>The <strong>ACVF</strong> is given by</p>
<p><span class="math display">\[
\begin{aligned}
\gamma(k) &amp;= E[X_t X_{t+k}] \\
          &amp;= E[(\Sigma \alpha^i Z_{t-i})(\Sigma \alpha^j Z_{t+k-j})] \\
          &amp;= \sigma^2_Z \sum_{i = 0}^{\infty} \alpha^i \alpha^{k+i} \qquad \text{for} \; k \ge 0 \\
          &amp;= \alpha^k \sigma^2_Z/(1- \alpha^2) \qquad \text{provided } |\alpha| &lt; 1 \\
          &amp;= \alpha^k \sigma^2_X
          \end{aligned}
\]</span></p>
<p>For <span class="math inline">\(k&lt;0\)</span>, we find <span class="math inline">\(\gamma(k) = \gamma(-k)\)</span>.
We see that <span class="math inline">\(\gamma(k)\)</span> is independent of <span class="math inline">\(t\)</span>, thus the <span class="math inline">\(\text{AR}\)</span> process of order 1 is second-order stationary given that <span class="math inline">\(|\alpha| &lt;1\)</span>.
The <strong>ACF</strong> is given by</p>
<p><span class="math display">\[
\rho(k) = \alpha^k \qquad k = 0, 1, 2, \dots
\]</span></p>
<p>The <strong>ACF</strong> may also be obtained more simply by assuming <em>a priori</em> that the process is stationary, in which case <span class="math inline">\(E(X_t)\)</span> must be zero, Multiply through equation (4.4.4.1.1) by <span class="math inline">\(X_{t-k}\)</span> and take expectations.
Then we find, for <span class="math inline">\(k&gt;0\)</span>, that</p>
<p><span class="math display">\[
\gamma(-k) = \alpha \gamma(-k+1)
\]</span></p>
<p>assuming that <span class="math inline">\(E(Z_t X_{t-k})=0\)</span> for <span class="math inline">\(k&gt;0\)</span>.
Since <span class="math inline">\(\gamma(k)\)</span> is an even function, we must also have</p>
<p><span class="math display">\[
\gamma(k) = \alpha \gamma(k-1) \qquad \text{for} \; k &gt; 0
\]</span></p>
<p>Now <span class="math inline">\(\gamma(0)=\sigma^2_X\)</span>, and so <span class="math inline">\(\gamma(k)=\alpha^k \sigma^2_X\)</span> for <span class="math inline">\(k \ge 0\)</span>.
This means that, keeping the same restrictions, <span class="math inline">\(\rho(k)=\alpha^k\)</span>.
But if <span class="math inline">\(|\alpha| = 1\)</span>, then <span class="math inline">\(|\rho(k)| = 1\)</span> for all <span class="math inline">\(k\)</span>, which is a degenerate case.
Thus <span class="math inline">\(|\alpha| &lt; 1\)</span> is required for a proper stationary process.</p>
<p>The above method of obtaining the <strong>ACF</strong> commonly used, over the assumption that the time series is stationary.</p>
<pre class="r"><code>oldpar &lt;- par(mfrow = c(3, 1), mar = c(3.1, 4.1, 2.1, 1.1))
set.seed(2021)

auto_reg &lt;- arima.sim(list(order = c(1,0,0), ar = 0.8), n = 200)
plot.ts(auto_reg, main = &quot;Autoregressive AR(1)&quot;, ylab = &quot;Value&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
acf(auto_reg)
par(mar = c(5.1, 4.1, 0.1, 1.1))
pacf(auto_reg)

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/stoch_autoreg-1.png" width="700" style="display: block; margin: auto;" /></p>
</div>
<div id="general-order-case-1" class="section level4" number="4.3.4.2">
<h4><span class="header-section-number">4.3.4.2</span> General-order case</h4>
<p>In the general-order case, the same property of the first-order case stands true: an <span class="math inline">\(\text{AR}\)</span> process of finite order can be represented as a <span class="math inline">\(\text{MA}\)</span> process of infinite order.
We can use the same methods as before, by successive substitution or by using the backward shift operator.
Then equation (4.3.4.1) may be written as</p>
<p><span class="math display">\[
(1-\alpha_1 B - \cdots -\alpha_p B^p)X_t = Z_t
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\begin{aligned}
X_t &amp;= Z_t/(1-\alpha_1 B - \cdots - \alpha_p B^p) \\
    &amp;= f(B) Z_t
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
f(B) &amp;= (1 - \alpha_1B - \cdots - \alpha_pB^p)^{-1} \\
     &amp;= (1 + \beta_1B+\beta_2B^2+\cdots)
\end{aligned}
\]</span></p>
<p>The relationship between the <span class="math inline">\(\alpha\)</span>s and the <span class="math inline">\(\beta\)</span>s may then be found.
Having expressed <span class="math inline">\(X_t\)</span> as a <span class="math inline">\(\text{MA}\)</span> process, it follows that <span class="math inline">\(E(X_t)=0\)</span>.
The variance is finite provided that <span class="math inline">\(\Sigma \beta^2_i\)</span> converges, and this is a necessary condition for stationarity.
The <strong>ACVF</strong> is given by</p>
<p><span class="math display">\[
\gamma(k)=\sigma^2_Z \sum_{i=0}^\infty \beta_i\beta_{i+k} \qquad \text{where} \; \beta_0 = 1
\]</span></p>
<p>We can simply state that if <span class="math inline">\(\Sigma |\beta_i|\)</span> converges, the process is stationary.</p>
<p><em>Yule-Walker equations</em></p>
<p>We can, in principle, find the <strong>ACF</strong> of the general-order <span class="math inline">\(\text{AR}\)</span> process using the above procedure, but the <span class="math inline">\(\{\beta_i\}\)</span> may be hard to find by algebraic methods.
We can simplify this by <strong>assuming</strong> the process is stationary and multiply through equation (4.3.4.1) by <span class="math inline">\(X_{t-k}\)</span>, take expectations, and divide by <span class="math inline">\(\sigma^2_X\)</span>, assuming that the variance of <span class="math inline">\(X_t\)</span>, is finite.
Then, using the fact that <span class="math inline">\(\rho(k) = \rho(-k)\)</span> for all <span class="math inline">\(k\)</span>, we have</p>
<p><span class="math display">\[
\rho(k) = \alpha_1 \rho(k-1) + \cdots + \alpha_p\rho(k-p) \qquad \text{for all } k &gt; 0
\]</span></p>
<p>These equations composes the group of equations called the Yule-Walker equations named after G.
Yule and G.
Walker.
Which has the general form</p>
<p><span class="math display">\[
\rho(k) = A_1 \pi_1^{|k|} + \cdots + A_p\pi_p^{|k|}
\]</span></p>
<p>where <span class="math inline">\(\{\pi_i\}\)</span> are the roots of the auxiliary equation</p>
<p><span class="math display">\[
y^p-\alpha_1y^{p-1} - \cdots -\alpha_p=0
\]</span></p>
<p>The constants <span class="math inline">\(\{A_i\}\)</span> must satisfy the initial condition of that <span class="math inline">\(\Sigma A_i = 1\)</span>, depending on <span class="math inline">\(\rho(0)=1\)</span>.</p>
<p><em>Stationarity conditions</em></p>
<p>From the general form of <span class="math inline">\(\rho(k)\)</span>, it is clear that <span class="math inline">\(\rho(k)\)</span> tends to zero as <span class="math inline">\(k\)</span> increases provided that <span class="math inline">\(|\pi_i| &lt; 1\)</span> for all <span class="math inline">\(i\)</span>, and this is enough for the <span class="math inline">\(\text{AR}(p)\)</span> process to be stationary.</p>
<p>We can also say that if the roots of the following equation lie outside the unit circle the process is stationary (Box and Jenkins, 1970, Section 3.2)</p>
<p><span class="math display">\[
\phi(B)=1-\alpha_1B-\cdots -\alpha_pB^p=0 \tag{4.3.4.2.1}
\]</span></p>
<p>Of particular interest is the <span class="math inline">\(AR(2)\)</span> process, when <span class="math inline">\(\pi_1\)</span>, <span class="math inline">\(\pi_2\)</span> are the roots of the quadratic equation</p>
<p><span class="math display">\[
y^2 - \alpha_1y - \alpha_2 = 0
\]</span></p>
<p>Thus if <span class="math inline">\(|\pi_i| &lt; 1\)</span> if</p>
<p><span class="math display">\[
\left|\frac{\alpha_{1} \pm \sqrt{(\alpha_{1}^{2}+4 \alpha_{2})}}{2}\right|&lt;1
\]</span></p>
<p>from which the stationarity region is the triangular region satisfying</p>
<p><span class="math display">\[
\begin{aligned}
\alpha_1 + \alpha_2 &amp;&lt; 1 \\
\alpha_1 - \alpha_2 &amp;&gt; -1 \\
\alpha_2 &amp;&gt; -1
\end{aligned}
\]</span></p>
<p>The roots are real if <span class="math inline">\(\alpha^2_1 + 4\alpha_2 &gt; 0\)</span>, in which case the <strong>ACF</strong> decreases exponentially with <span class="math inline">\(k\)</span>, but the roots are complex if <span class="math inline">\(\alpha^2_1 + 4\alpha_2 &lt; 0\)</span>, in which case we find that the <strong>ACF</strong> is a damped sinusoidal wave.</p>
<p>When the roots are real, the constants <span class="math inline">\(A_1\)</span>, <span class="math inline">\(A_2\)</span> are also real and are found as follows.
Since <span class="math inline">\(\rho(0)=1\)</span>, we have</p>
<p><span class="math display">\[
A_1 + A_2 = 1
\]</span></p>
<p>From the first of the Yule-Walker equations, we have</p>
<p><span class="math display">\[
\begin{aligned}
\rho(1) &amp;= \alpha_1 \rho(0) + \alpha_2 \rho(-1) \\
        &amp;= \alpha_1 + \alpha_2 \rho(1) \\
\end{aligned}
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[
\begin{aligned}
\rho(1) &amp;= \alpha_1/(1-\alpha_2) \\
        &amp;= A_1 \pi_1 + A_2 \pi_2 \\
        &amp;= A_1 \pi_1 + (1 - A_1) \pi_2 \\
\end{aligned}
\]</span></p>
<p>Hence we find</p>
<p><span class="math display">\[
\begin{aligned}
A_1 &amp;= [\alpha_1/(1-\alpha_2)-\pi_2]/(\pi_1- \pi_2) \\
A_2 &amp;= 1-A_1
\end{aligned}
\]</span></p>
<p>The <span class="math inline">\(\text{AR}\)</span> processes are useful to model time series that we assume that the current observation depends on the immediate past values plus a random error.
It is usual to assume that the mean of the process is zero, as a way to improve computation.
In reality this is not true for the observed values.
We can turn the process a zero-mean process by rewriting equation (4.3.4.1) in the form</p>
<p><span class="math display">\[
X_t - \mu=\alpha_1 (X_{t-1} -\mu)+ \cdots + \alpha_p (X_{t-p} - \mu) + Z_t
\]</span></p>
<p>This does not affect the <strong>ACF</strong>.</p>
</div>
</div>
<div id="mixed-textarma-models" class="section level3" number="4.3.5">
<h3><span class="header-section-number">4.3.5</span> Mixed <span class="math inline">\(\text{ARMA}\)</span> models</h3>
<p>Using the previous knowledge of <span class="math inline">\(\text{MA}\)</span> and <span class="math inline">\(\text{AR}\)</span> processes, and their relations, we can combine both into a mixed autoregressive/moving-average process containing <span class="math inline">\(p\)</span> <span class="math inline">\(\text{AR}\)</span> terms and <span class="math inline">\(q\)</span> <span class="math inline">\(\text{MA}\)</span> terms.
This is the <span class="math inline">\(\text{ARMA}\)</span> process of order <span class="math inline">\((p, q)\)</span>, and it is given by</p>
<p><span class="math display">\[
X_t=\alpha_1X_{t-1}+ \cdots + \alpha_pX_{t-p} + Z_t + \beta_1Z_{t-1}+ \cdots + \beta_qZ_{t-q} \tag{4.3.5.1}
\]</span></p>
<p>Using the backward shift operator <span class="math inline">\(B\)</span>, equation (4.3.5.1) may be written in the form</p>
<p><span class="math display">\[
\phi(B)X_t = \theta(B)Z_t \tag{4.3.5.1a}
\]</span></p>
<p>where <span class="math inline">\(\phi(B)\)</span>, <span class="math inline">\(\theta(B)\)</span> are polynomials of order <span class="math inline">\(p\)</span>, <span class="math inline">\(q\)</span> respectively, such that</p>
<p><span class="math display">\[
\phi(B) = 1 - \alpha_1B-\cdots-\alpha_pB^p
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\theta(B) = 1 + \beta_1B+\cdots+\beta_qB^q
\]</span></p>
<div id="stationarity-and-invertibility-conditions" class="section level4" number="4.3.5.1">
<h4><span class="header-section-number">4.3.5.1</span> Stationarity and invertibility conditions</h4>
<p>The values of <span class="math inline">\(\{\alpha_i\}\)</span> which makes the <span class="math inline">\(\text{AR}\)</span> process stationary must be such that the roots of</p>
<p><span class="math display">\[
\phi(B) = 0
\]</span></p>
<p>lie outside the unit circle.</p>
<p>While the values of <span class="math inline">\(\{\beta_i\}\)</span> which makes the <span class="math inline">\(\text{MA}\)</span> process invertible are such that the roots of</p>
<p><span class="math display">\[
\theta(B) = 0
\]</span></p>
<p>lie outside the unit circle.</p>
<p>In this article we will not explain how to compute the <strong>ACF</strong> for an <span class="math inline">\(\text{ARMA}\)</span> process.
It is not obscure, but tedious.
You can find it on Box and Jenkins, 1970, Section 3.4).</p>
<p>But in short, for the <span class="math inline">\(\text{ARMA}(1, 1)\)</span> case, we have</p>
<p><span class="math display">\[
\rho(k) = \alpha\rho(k − 1) \qquad k \ge 2
\]</span></p>
<pre class="r"><code>set.seed(2021)
oldpar &lt;- par(mfrow = c(3, 1), mar = c(3.1, 4.1, 2.1, 1.1))

stoch_arma &lt;- arima.sim(list(order = c(1,0,1), ma = 0.8, ar = 0.8), n = 200)
plot.ts(stoch_arma, main = &quot;ARMA AR(1) MA(1)&quot;, ylab = &quot;Value&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
acf(stoch_arma)
par(mar = c(5.1, 4.1, 0.1, 1.1))
pacf(stoch_arma)

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/stoch_arma-1.png" width="700" style="display: block; margin: auto;" /></p>
<p>And the general case, we have</p>
<p><span class="math display">\[
\rho(k) = \frac{(1+\alpha \beta)(\alpha + \beta)}{1+2\alpha \beta+\beta^2} \alpha^{k-1} \qquad k \ge 1
\]</span></p>
<p>Our primary objective here is to see how we can describe a stationary time series using an <span class="math inline">\(\text{ARMA}\)</span> model using fewer parameters than if we used a <span class="math inline">\(\text{MA}\)</span> or <span class="math inline">\(\text{AR}\)</span> process alone.
This is also known as the <strong>Principle of Parsimony</strong>, where it means that we want a model with fewer parameters possible that still represents our data adequately.</p>
</div>
<div id="the-textar-and-textma-representations" class="section level4" number="4.3.5.2">
<h4><span class="header-section-number">4.3.5.2</span> The <span class="math inline">\(\text{AR}\)</span> and <span class="math inline">\(\text{MA}\)</span> representations</h4>
<p>It is clearer to express an <span class="math inline">\(\text{ARMA}\)</span> model as a pure <span class="math inline">\(\text{MA}\)</span> process in the form</p>
<p><span class="math display">\[
X_t=\psi(B)Z_t \tag{4.3.5.1b}
\]</span></p>
<p>Or a pure <span class="math inline">\(\text{AR}\)</span> process in the form</p>
<p><span class="math display">\[
\pi(B)X_t=Z_t \tag{4.3.5.1c}
\]</span></p>
<p>where <span class="math inline">\(\psi(B)=\Sigma_{i&gt;0} \psi_i B^i\)</span> is the <span class="math inline">\(\text{MA}\)</span> operator that may be of infinite order.
The <span class="math inline">\(\psi\)</span> weights can be used to make predictions and assess the validity of the model.</p>
<p>Moving around the functions we can see lots of equities: <span class="math inline">\(\psi(B)=\theta(B)/\phi(B)\)</span> and <span class="math inline">\(\pi(B)=\phi(B)/\theta(B)\)</span>.
Also <span class="math inline">\(\pi(B)\psi(B)=1\)</span> and <span class="math inline">\(\psi(B)\phi(B)=\theta(B)\)</span>.</p>
<p>By convention we write <span class="math inline">\(\pi(B) = 1 - \sum_{i \ge 1}\pi_i B^i\)</span>, since the natural way to write an <span class="math inline">\(\text{AR}\)</span> model is in the form</p>
<p><span class="math display">\[
X_t=\sum_{i=1}^\infty \pi_i X_{t-i} + Z_t
\]</span></p>
</div>
</div>
<div id="integrated-textarima-models" class="section level3" number="4.3.6">
<h3><span class="header-section-number">4.3.6</span> Integrated <span class="math inline">\(\text{ARIMA}\)</span> models</h3>
<p>In real life, most of the time series we have are non-stationary.
This means we have to first remove this source of variation before working with the models we have seen until now, or, we use another composition that already takes in account the non-stationarity.
As suggested in section <a href="#differencing">3.2.3</a>, we can difference the time series to turn it stationary.</p>
<p>Formally we replace <span class="math inline">\(X_t\)</span> by <span class="math inline">\(\nabla^d X_t\)</span> where <span class="math inline">\(d\)</span> is how many times we take the difference (<span class="math inline">\(\nabla\)</span>) of <span class="math inline">\(X_t\)</span>.
This model is called an “integrated” model because the fitted model on the differenced data needs to be summed (or “integrated”) to fit the original data.</p>
<p>Here we define the <span class="math inline">\(\text{ARIMA}\)</span> model as</p>
<p><span class="math display">\[
W_t = \nabla^d X_t = (1-B)^d X_t \qquad d \in \mathbb{N}_0
\]</span></p>
<p>the general autoregressive integrated moving average process (abbreviated <span class="math inline">\(\text{ARIMA}\)</span> process) is of the form</p>
<p><span class="math display">\[
W_t = \alpha_1 W_{t-1} + \cdots + \alpha_p W_{t-p} + Z_t + \cdots + \beta_qZ_{t-q} \tag{4.3.6.1}
\]</span></p>
<p>By analogy with equation (4.3.5.1a), we may write equation (4.3.6.1) in the form</p>
<p><span class="math display">\[
\phi(B)W_t = \theta(B)Z_t \tag{4.3.6.1a}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\phi(B)(1-B)^dX_t=\theta(B)Z_t \tag{4.3.6.1b}
\]</span></p>
<pre class="r"><code>oldpar &lt;- par(mfrow = c(3, 1), mar = c(3.1, 4.1, 2.1, 1.1))
set.seed(2021)

stoch_arma &lt;- arima.sim(list(order = c(2,1,2), ma = c(-0.2279, 0.2488), ar = c(0.8897, -0.4858)), n = 200)
plot.ts(stoch_arma, main = &quot;ARIMA(2, 1, 2)&quot;, ylab = &quot;Value&quot;)
par(mar = c(5.1, 4.1, 0.1, 1.1))
acf(stoch_arma)
par(mar = c(5.1, 4.1, 0.1, 1.1))
pacf(stoch_arma)

par(oldpar)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/stoch_arima-1.png" width="700" style="display: block; margin: auto;" /></p>
<p>Thus we have an <span class="math inline">\(\text{ARIMA}\)</span> process of order <span class="math inline">\((p,d,q)\)</span>.
The model for <span class="math inline">\(X_t\)</span> is clearly non-stationary, as the <span class="math inline">\(\text{AR}\)</span> operator <span class="math inline">\(\phi(B)(1 - B)^d\)</span> has <span class="math inline">\(d\)</span> roots on the unit circle.
Just for curiosity, see that the random walk process can be modeled in the form of an <span class="math inline">\(\text{ARIMA}(0, 1, 0)\)</span> process.</p>
</div>
</div>
</div>
<div id="glossary" class="section level1 unnumbered">
<h1>Glossary</h1>
<dl>
<dt><span class="math inline">\(E\)</span></dt>
<dd><p>Expected value or expectation</p>
</dd>
<dt><span class="math inline">\(B\)</span></dt>
<dd><p>Backwards shift operator: <span class="math inline">\(BX_t = X_{t-1}\)</span></p>
</dd>
<dt><span class="math inline">\(\tau\)</span></dt>
<dd><p>Period of time, lag</p>
</dd>
<dt><span class="math inline">\(\lambda\)</span></dt>
<dd><p>Transformation parameter or some other constant</p>
</dd>
<dt><span class="math inline">\(\mu\)</span></dt>
<dd><p>Mean</p>
</dd>
<dt><span class="math inline">\(\sigma^2\)</span></dt>
<dd><p>Variance</p>
</dd>
<dt><span class="math inline">\(r_k\)</span></dt>
<dd><p>Correlogram coefficient at lag index ‘k’</p>
</dd>
<dt><span class="math inline">\(\nabla\)</span></dt>
<dd><p>Difference operator</p>
</dd>
<dt><span class="math inline">\(\gamma(\tau)\)</span></dt>
<dd><p>Autocovariance of a period of time: <span class="math inline">\(\gamma(t_1, t_2)\)</span></p>
</dd>
<dt><span class="math inline">\(\rho(\tau)\)</span></dt>
<dd><p>Autocorrelation of a period of time: <span class="math inline">\(\gamma(\tau)/\gamma(0)\)</span></p>
</dd>
</dl>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-chatfield1996" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Chatfield, C.: The Analysis of Time Series: An Introduction. Chapman and Hall/CRC (1996).</div>
</div>
<div id="ref-loynes2005" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Loynes, R.M.: Slutzky-Yule Effect, <a href="http://dx.doi.org/10.1002/0470011815.b2a12064">http://dx.doi.org/10.1002/0470011815.b2a12064</a>, (2005). <a href="https://doi.org/10.1002/0470011815.b2a12064">https://doi.org/10.1002/0470011815.b2a12064</a>.</div>
</div>
<div id="ref-montgomery2012" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Montgomery, D.: Introduction to Statistical Quality Control. Wiley (2012).</div>
</div>
<div id="ref-siegel2016" class="csl-entry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Siegel, A.F.: Time Series. In: Practical Business Statistics. pp. 431–466 Elsevier (2016). <a href="https://doi.org/10.1016/b978-0-12-804250-2.00014-6">https://doi.org/10.1016/b978-0-12-804250-2.00014-6</a>.</div>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Usually, analog signals are “digitized” by reading the value at discrete intervals.
The pitfalls of this approach are beyond this article.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Treatment of outliers is another field of research and out of the scope of this article.
An outlier may be a perfectly valid but extreme value, or a sensor that has gone entirely wild, or even a strange observation that may or not repeat for some reason.
So common sense is as important as the theory.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>For those looking for more trouble: the moving average process, despite what was said about the moving average algorithm, is, in fact, a (weighted) moving average of a random process!
Try this at home: simulate a <span class="math inline">\(\text{MA}(2)\)</span> process with coefficients 1 (so we skip the weighted part).
Then, apply a rollmean() function with window 3 to the random process used in the simulation (yes, window is 3, because we need to consider the current value).
Then compare both resulting time series.
If all correct, you will have the same values.
Tip: you may need to align the starting value and multiply the rollmean() output by 2 (the order of the moving average process).<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>In the R language, we often see the <strong>inverse</strong> roots when we plot an ARIMA model.
In this case, we will see the roots <strong>inside</strong> the unit circle when the process is invertible.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Remember the <em>power series</em> where <span class="math inline">\(\frac{a}{1-x} = \sum_{n=0}^\infty ax^n\)</span>.
Thus we have <span class="math inline">\(\frac{Z_t}{1-(\alpha B)} = \sum_{n=0}^\infty Z_t(\alpha B)^n\)</span><a href="#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
